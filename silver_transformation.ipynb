{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "347a62a9-9a57-463a-b89b-44578d1a21cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from silver_tbl_columns_dict import silver_tbl_dict\n",
    "\n",
    "weather_df = spark.table('`nyc_taxis_weather`.bronze_nyc_taxis.b_weather_oct_dec_2024').drop(col(\"_rescued_data\"))\n",
    "taxis_df = spark.table('`nyc_taxis_weather`.bronze_nyc_taxis.b_nyc_taxis_oct_dec_2024').drop(col(\"_rescued_data\"))\n",
    "lookup_df = spark.table('`nyc_taxis_weather`.bronze_nyc_taxis.b_taxi_zone_lookup').drop(col(\"_rescued_data\"))\n",
    "#weather codes .csv table uploaded manually (reference: https://dev.meteostat.net/formats.html#weather-condition-codes)\n",
    "w_code_df = spark.table('`nyc_taxis_weather`.bronze_nyc_taxis.b_weather_codes').drop(col(\"_rescued_data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0e46ed5-453e-4212-baeb-572861e8f685",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import broadcast\n",
    "from pyspark.sql.functions import col, date_trunc\n",
    "\n",
    "#Using broadcast join because location table contains ~250 records so it is small enough to be cached and joined\n",
    "# loc_dict = broadcast(lookup_df)\n",
    "# weather_brd = broadcast(weather_df)\n",
    "\n",
    "#Truncating pickup time to enable joining weather data (weather data available for full hours)\n",
    "dt_taxis_hour = taxis_df.withColumn(\"pickup_hour\", date_trunc(\"hour\", col(\"tpep_pickup_datetime\")))\n",
    "\n",
    "#Using broadcast join because weather table contains ~720 records so it is small enough to be cached and joined\n",
    "dt_taxis_weather = dt_taxis_hour.join(weather_df, dt_taxis_hour.pickup_hour == weather_df.time, \"left\").drop(col(\"datetime\"), col(\"time\"), col(\"pickup_hour\"))\n",
    "\n",
    "#Joining dictionary table to create final result table ready to be analysed\n",
    "result_table = dt_taxis_weather.join(lookup_df.withColumnRenamed(\"LocationID\", \"PULocationID_tmp\")\n",
    "                                    .withColumnRenamed(\"Borough\", \"PUBorough\").drop(col(\"service_zone\"))\n",
    "                                    .withColumnRenamed(\"Zone\", \"PUZone\"),dt_taxis_weather.PULocationID == col(\"PULocationID_tmp\"),\"left\").join(lookup_df.withColumnRenamed(\"LocationID\", \"DOLocationID_tmp\")\n",
    "                                    .withColumnRenamed(\"Borough\", \"DOBorough\")\n",
    "                                    .withColumnRenamed(\"Zone\", \"DOZone\"),dt_taxis_weather.DOLocationID == col(\"DOLocationID_tmp\"),\"left\").join(w_code_df,dt_taxis_weather.coco == col(\"w_code\"), \"left\")\n",
    "                                    \n",
    "#adding fare duration and dropping unnecessary post-join coilumns                                   \n",
    "result_table = result_table.withColumn(\"trip_duration\", (col(\"tpep_dropoff_datetime\").cast(\"long\") - col(\"tpep_pickup_datetime\").cast(\"long\")) / 60).drop(\"PULocationID_tmp\", \"DOLocationID_tmp\", \"w_code\", \"coco\", \"store_and_fwd_flag\", \"PULocationID\", \"DOLocationID\", \"congestion_surcharge\")\n",
    "\n",
    "result_table = result_table.withColumnsRenamed(silver_tbl_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93483b67-8b9a-451e-950b-4a7939217b1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "#saving table with overwrite and mergeSchema which allows for schema evolution, in latest version of table schema was updated along with the partitioning by pickup day, few columns were renamed for better naming clarity\n",
    "\n",
    "result_table.withColumn(\n",
    "    \"pickup_date\", \n",
    "    to_date(\"pickup_datetime\")\n",
    ").write.partitionBy(\n",
    "    \"pickup_date\"\n",
    ").option(\n",
    "    \"mergeSchema\", \"true\"\n",
    ").option(\n",
    "    \"overwriteSchema\", \"true\"\n",
    ").mode(\n",
    "    \"overwrite\"\n",
    ").saveAsTable(\n",
    "    \"`nyc_taxis_weather`.silver_nyc_taxis.s_nyc_taxis_weather_oct_dec\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8839288345435750,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_transformation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
