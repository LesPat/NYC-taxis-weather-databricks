{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "347a62a9-9a57-463a-b89b-44578d1a21cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from silver_tbl_columns_dict import silver_tbl_dict\n",
    "\n",
    "weather_df = spark.table('`nyc_taxis_weather`.bronze_nyc_taxis.b_weather_oct_dec_2024').drop(col(\"_rescued_data\"))\n",
    "taxis_df = spark.table('`nyc_taxis_weather`.bronze_nyc_taxis.b_nyc_taxis_oct_dec_2024').drop(col(\"_rescued_data\"))\n",
    "lookup_df = spark.table('`nyc_taxis_weather`.bronze_nyc_taxis.b_taxi_zone_lookup').drop(col(\"_rescued_data\"))\n",
    "#weather codes .csv table uploaded manually (reference: https://dev.meteostat.net/formats.html#weather-condition-codes)\n",
    "w_code_df = spark.table('`nyc_taxis_weather`.bronze_nyc_taxis.b_weather_codes').drop(col(\"_rescued_data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0e46ed5-453e-4212-baeb-572861e8f685",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, date_trunc, round\n",
    "\n",
    "#Truncating pickup time to enable joining weather data (weather data available for full hours)\n",
    "dt_taxis_hour = taxis_df.withColumn(\"pickup_hour\", date_trunc(\"hour\", col(\"tpep_pickup_datetime\")))\n",
    "\n",
    "#Joining weather data to taxi data by hour truncated above\n",
    "dt_taxis_weather = dt_taxis_hour.join(weather_df, dt_taxis_hour.pickup_hour == weather_df.time, \"left\").drop(col(\"datetime\"), col(\"time\"), col(\"pickup_hour\"))\n",
    "\n",
    "#Joining dictionary table to create final result table ready to be analysed\n",
    "result_table = dt_taxis_weather.join(lookup_df.withColumnRenamed(\"LocationID\", \"PULocationID_tmp\")\n",
    "                                    .withColumnRenamed(\"Borough\", \"PUBorough\").drop(col(\"service_zone\"))\n",
    "                                    .withColumnRenamed(\"Zone\", \"PUZone\"),dt_taxis_weather.PULocationID == col(\"PULocationID_tmp\"),\"left\").join(lookup_df.withColumnRenamed(\"LocationID\", \"DOLocationID_tmp\")\n",
    "                                    .withColumnRenamed(\"Borough\", \"DOBorough\")\n",
    "                                    .withColumnRenamed(\"Zone\", \"DOZone\"),dt_taxis_weather.DOLocationID == col(\"DOLocationID_tmp\"),\"left\").join(w_code_df,dt_taxis_weather.coco == col(\"w_code\"), \"left\")\n",
    "                                    \n",
    "#Adding fare duration and dropping unnecessary post-join columns                                   \n",
    "result_table = result_table.withColumn(\"trip_duration\", round((col(\"tpep_dropoff_datetime\").cast(\"long\") - col(\"tpep_pickup_datetime\").cast(\"long\")) / 60, 2)).drop(\"PULocationID_tmp\", \"DOLocationID_tmp\", \"w_code\", \"coco\", \"store_and_fwd_flag\", \"PULocationID\", \"DOLocationID\", \"congestion_surcharge\")\n",
    "\n",
    "#Renaming columns for clarity\n",
    "result_table = result_table.withColumnsRenamed(silver_tbl_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93483b67-8b9a-451e-950b-4a7939217b1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "#saving table with overwrite and mergeSchema which allows schema evolution, in latest version of table schema was updated along with the partitioning by pickup day, few columns were renamed for better naming clarity\n",
    "\n",
    "result_table.withColumn(\"pickup_date\", to_date(\"pickup_datetime\")).write.partitionBy(\"pickup_date\").option(\"mergeSchema\", \"true\").option(\"overwriteSchema\", \"true\").mode(\"overwrite\").saveAsTable(\"`nyc_taxis_weather`.silver_nyc_taxis.s_nyc_taxis_weather_oct_dec\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8839288345435750,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_main_transformation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
